<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
	<title></title>
	<meta name="generator" content="LibreOffice 6.4.6.2 (Linux)"/>
	<meta name="created" content="2021-03-17T09:21:20.310823925"/>
	<meta name="changed" content="2021-03-19T11:51:42.468620150"/>
	<style type="text/css">
		@page { size: 21cm 29.7cm; margin: 2cm }
		p { margin-bottom: 0.25cm; line-height: 115%; background: transparent }
		pre { background: transparent }
		pre.western { font-family: "Liberation Mono", monospace; font-size: 10pt }
		pre.cjk { font-family: "Noto Sans Mono CJK SC", monospace; font-size: 10pt }
		pre.ctl { font-family: "Liberation Mono", monospace; font-size: 10pt }
		td p { background: transparent }
		a:link { color: #000080; so-language: zxx; text-decoration: underline }
		a:visited { color: #800000; so-language: zxx; text-decoration: underline }
	</style>
</head>
<body lang="en-IN" link="#000080" vlink="#800000" dir="ltr"><p style="margin-bottom: 0cm; line-height: 100%">
<b>I) Summary:</b></p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%"><b>1. Problem
Definition:</b></p>
<p style="margin-bottom: 0cm; line-height: 100%">A lot has been said
during the past several years about how precision medicine and, more
concretely, how genetic testing is going to disrupt the way diseases
like cancer are treated.</p>
<p style="margin-bottom: 0cm; line-height: 100%">But this is only
partially happening due to the huge amount of manual work still
required. 
</p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%">Memorial Sloan
Kettering Cancer Center (MSKCC) launched this competition to take
personalized medicine to its full potential.</p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%">A cancer tumor can
have thousands of genetic mutations. And distinguishing the mutations
that contribute to tumor growth is challenging. 
</p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%">Currently this
interpretation of genetic mutations is being done manually. This is a
very time-consuming task where a clinical pathologist has to manually
review and classify every single genetic mutation based on evidence
from text-based clinical literature.</p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%"><b>Workflow is as
follows:</b></p>
<p style="margin-bottom: 0cm; line-height: 100%">i. A molecular
pathologist selects a list of genetic variations of interest that
he/she want to analyze</p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%">ii. The molecular
pathologist searches for evidence in the medical literature that
somehow are relevant to the genetic variations of interest.</p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%">iii. Finally this
molecular pathologist spends a huge amount of time analyzing the
evidence related to each of the variations to classify them.</p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%">Our goal here is to
replace step 3 by a machine learning model. The molecular pathologist
will still have to decide which variations are of interest, and also
collect the relevant evidence for them. But the last step, which is
also the most time consuming, will be fully automated.</p>
<p style="margin-bottom: 0cm; line-height: 100%"><br/>

</p>
<p style="margin-bottom: 0cm; line-height: 100%"><b>2. Objective:</b></p>
<p style="margin-bottom: 0cm; line-height: 100%">Predict the
probability of each data-point belonging to each of the nine classes.</p>
<p><br/>
<br/>

</p>
<p><b>3.</b><b>Constraints:</b> 
</p>
<p>* Interpretability,  * Class probabilities are needed, * Penalize
the errors in class probabilites =&gt; Metric is Log-loss. * No
Latency constraints.</p>
<p><b>4. EDA:</b></p>
<p>i. More number of data points belong to class 7, 4, 1 and 2 .</p>
<p><img src="One_Page_Summary_of_PCD_1.4_html_6419f45c18ecf892.png" name="Image4" align="left" width="332" height="222" border="0"/>
<br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p>ii. Prediction of class using Random model and plot confusion
matrix.</p>
<p>	Log loss on CV data using Random model = 2.459</p>
<p>	Log loss on Test Data using Random model = 2.498</p>
<p><br/>
<br/>

</p>
<p><b>5. Univariate Analysis of Gene Feature:</b></p>
<p><b>Observations:</b></p>
<p>i. Top 50 genes contribute to 70% of data and remaining genes
contribute 30% of data.</p>
<p>ii. Lots of genes occur very few times and very few genes occur
lot of times.</p>
<p>iii. How good Gene feature in predicting class label? : Train
Logistic Regression model using only the “Gene Feature” .</p>
<p> <img src="One_Page_Summary_of_PCD_1.4_html_89cfbef884fc0575.png" name="Image1" align="left" width="570" height="252" border="0"/>

                                                                     
                      
</p>
<p><b>iv. Stability of Gene feature : </b>
</p>
<p>In test data out of 665 data points , 664 are present in the train
dataset also.</p>
<p>In CV data out of 532 data points, 519 are present in the train
dataset also.</p>
<p><br/>
<br/>

</p>
<p><b>6</b><b>. Univariate Analysis of </b><b>Variation</b><b>
Feature:</b></p>
<p><span style="font-weight: normal">i. </span><span style="font-weight: normal">How
good Gene feature in predicting class label? : Train Lo</span><span style="font-weight: normal">gistic
Regression model using only the “</span><span style="font-weight: normal">Veriation</span><span style="font-weight: normal">
Feature” .</span></p>
<p><span style="font-weight: normal">T</span><span style="font-weight: normal">rain
Logistic Regression model with “Variation Feature” alone.</span></p>
<p> <img src="One_Page_Summary_of_PCD_1.4_html_d43bc0bcd2213e51.png" name="Image2" align="left" width="643" height="274" border="0"/>

                                               
</p>
<p><b>i</b><b>i</b><b>. </b><b>Stability of </b><b>Variation</b><b>
feature :</b></p>
<pre class="western" style="font-weight: normal">In test data 66 out of 665 data points are present in train data.
<span style="font-weight: normal">In CV data </span>51 out of 532 data points are present in Train data.

</pre><p>
<b>7.</b><b> Univariate Analysis of </b><b>Gene</b><b> Feature:</b></p>
<p style="font-weight: normal">i. Total number of unique words in
train data : 53491</p>
<p><span style="font-weight: normal">ii. </span><span style="font-weight: normal">T</span><span style="font-weight: normal">rain
Logistic Regression model with “</span><span style="font-weight: normal">Text</span><span style="font-weight: normal">
Feature” alone.</span></p>
<p style="font-weight: normal"> <img src="One_Page_Summary_of_PCD_1.4_html_e3078bc305894335.png" name="Image3" align="left" width="620" height="280" border="0"/>

</p>
<p><span style="font-weight: normal">i</span><b>i</b><b>i</b><b>.
</b><b>Stability of </b><b>Text</b><b> feature :</b></p>
<p>97.148 % of word of test data appeared in train data</p>
<p>97.602 % of word of Cross Validation appeared in train data</p>
<p><br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p><b>8. </b><b>Combining all the features </b><b>using hstack and
Training the different models with data.</b></p>
<table width="642" cellpadding="4" cellspacing="0">
	<col width="97"/>

	<col width="99"/>

	<col width="100"/>

	<col width="99"/>

	<col width="99"/>

	<col width="98"/>

	<tr valign="top">
		<td width="97" style="border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0.1cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			<font color="#81d41a"><b>Model</b></font></p>
		</td>
		<td width="99" style="border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0.1cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			<font color="#81d41a"><b>Hyperparameter (alpha)</b></font></p>
		</td>
		<td width="100" style="border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0.1cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			<font color="#81d41a"><b>Logloss (Train data)</b></font></p>
		</td>
		<td width="99" style="border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0.1cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			<font color="#81d41a"><b>Logloss (CV data)</b></font></p>
		</td>
		<td width="99" style="border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0.1cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			<font color="#81d41a"><b>Logloss (Test Data)</b></font></p>
		</td>
		<td width="98" style="border: 1px solid #000000; padding: 0.1cm"><p>
			<font color="#81d41a"><b>% Mis-classified points</b></font></p>
		</td>
	</tr>
	<tr valign="top">
		<td width="97" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			Naive Bayes (One hot encoding)</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1e-5</p>
		</td>
		<td width="100" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.893</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.289</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.288</p>
		</td>
		<td width="98" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 1px solid #000000; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0.1cm"><p>
			38.909</p>
		</td>
	</tr>
	<tr valign="top">
		<td width="97" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			K-NN (Response Coding)</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			41 (nearest neighbors)</p>
		</td>
		<td width="100" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.840</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.073</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.149</p>
		</td>
		<td width="98" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 1px solid #000000; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0.1cm"><p>
			39.285</p>
		</td>
	</tr>
	<tr valign="top">
		<td width="97" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			Logistic Regression (Balancing + One hot encoding)</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.001</p>
		</td>
		<td width="100" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.513</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.198</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.136</p>
		</td>
		<td width="98" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 1px solid #000000; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0.1cm"><p>
			34.962</p>
		</td>
	</tr>
	<tr valign="top">
		<td width="97" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			Logistic Regression (without balancing + One hot encoding)</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.001</p>
		</td>
		<td width="100" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.503</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.194</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.160</p>
		</td>
		<td width="98" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 1px solid #000000; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0.1cm"><p>
			35.714</p>
		</td>
	</tr>
	<tr valign="top">
		<td width="97" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			Lineara SVM (One hot encoding + Balancing)</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.001</p>
		</td>
		<td width="100" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.713</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.217</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.148</p>
		</td>
		<td width="98" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 1px solid #000000; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0.1cm"><p>
			37.406</p>
		</td>
	</tr>
	<tr valign="top">
		<td width="97" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			Random Forest (One hot encoding)</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			2000 (no of trees) and max depth = 10</p>
		</td>
		<td width="100" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.679</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.199</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.155</p>
		</td>
		<td width="98" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 1px solid #000000; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0.1cm"><p>
			40.225</p>
		</td>
	</tr>
	<tr valign="top">
		<td width="97" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p>
			Random Forest (Response encoding)</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			100 (no of trees) and max depth = 5</p>
		</td>
		<td width="100" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			0.0719</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.363</p>
		</td>
		<td width="99" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0cm"><p align="center">
			1.397</p>
		</td>
		<td width="98" style="border-top: none; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 1px solid #000000; padding-top: 0cm; padding-bottom: 0.1cm; padding-left: 0.1cm; padding-right: 0.1cm"><p>
			49.624</p>
		</td>
	</tr>
</table>
<p><br/>
<br/>

</p>
<p><br/>
<br/>

</p>
<p><b>Model Interpretation:</b></p>
<p style="font-weight: normal"><b>1.</b> <b>Naive Bayes :</b></p>
<p style="font-weight: normal">i.<b> </b>There is some gap between
Train and CV log loss, this might be because naive bayes is simpler
model.</p>
<p><span style="font-weight: normal">ii. </span><span style="font-weight: normal">We
can get Feaure imortance thus satisfies interpretability of the
model. </span><span style="font-weight: normal">Ie, gives the reason
for predicting the </span><span style="font-weight: normal">query
point belonging to a particular class.</span></p>
<p><b>2. K-NN (Response Coding):</b></p>
<p><span style="font-weight: normal">i. </span><span style="font-weight: normal">K-NN
can’t work with high dimensional data, hence use response coding
for the features.</span></p>
<p><span style="font-weight: normal">ii. There is some gap in train
log loss and cv log loss.</span></p>
<p><span style="font-weight: normal">Iii. </span><span style="font-weight: normal">More
easy to make mistakes in K-NN than Naive Bayes, because K-NN is not
interpretable.</span></p>
<p><b>3. Logistic Regression (Balancing + One hot encoding) :</b></p>
<p style="font-weight: normal">i. Logistic regression easily takes
high dimensional data, hence use onehot encoding of the features.</p>
<p style="font-weight: normal">ii. Is very interpretable and we can
get feature importance with absolute values of Weights.</p>
<p style="font-weight: normal">Iii. Class balancing helps to improve
the prediction even for minority class labeled data points.</p>
<p><b>4. Logistic Regression (without balancing + One hot encoding) :</b></p>
<p style="font-weight: normal">i. without class balancing precision
and recall values will be nearly to zero.</p>
<p style="font-weight: normal">ii. percentage of misclassified points
increases witout class balancing.</p>
<p><b>5. Lineara SVM (One hot encoding + Balancing) :</b></p>
<p><span style="font-weight: normal">i. When data is high
dimensional, then Linear SVM works very well.</span></p>
<p><span style="font-weight: normal">ii. Interpretability of the
model is good (very simillar to logistic Regression).</span></p>
<p><span style="font-weight: normal">Iii. </span><span style="font-weight: normal">We
are not using RBF SVM because it is not easily interpretable and also
we don’t know which kernel works well here.</span></p>
<p><b>6. </b><b>Random Forest </b><b>(One hot encoding) :</b></p>
<p style="font-weight: normal">i. Random Forest works well when the
dimensionality is small.</p>
<p style="font-weight: normal">ii. There are 2 hyper parameters,</p>
<p style="font-weight: normal">	a. Number of tress / number of base
estimators</p>
<p style="font-weight: normal">	b. Maximum depth of the tree.</p>
<p style="font-weight: normal">Iii. As number of trees increase,
model will generalize better.</p>
<p style="font-weight: normal">iv. In precision matrix we are getting
all diagonal elements =1 and also for all minor classes prediction
has improved.</p>
<p><b>7. Random Forest (Response encoding) :</b></p>
<p style="font-weight: normal"><br/>

</p>
<p style="font-weight: normal">i. The difference between train log
loss and cv log loss is very high and this means that model has
overfit.</p>
<p style="font-weight: normal"><br/>

</p>
<p style="font-weight: normal">ii. And also the percentage of
misclassified points is very high.</p>
<p><br/>

</p>
<p><font color="#5eb91e"><b>Conclusion:</b></font></p>
<p style="font-weight: normal">By comparing all the model log loss,
percentage of misclassified points and intepretability of the model,
we conclude that<b> Logistic Regression (One hot encoded features)
and with class balancing </b>gives the best results.</p>
<p style="font-weight: normal"><br/>
<br/>

</p>
<p><b>B</b><b>usiness Impact:</b></p>
<p style="font-weight: normal">1. As the Doctor receives a lot of
samples to test, in that some of them model predicts (with very high
probability of  belonging to a particular class), then it would
reduce the time taken by the doctor.</p>
<p style="font-weight: normal">2. Doctor can concentrate only on the
samples which the model can’t predict accurately, so that chance of
making error by doctor reduces drastically.</p>
<p style="font-weight: normal">3. We can make contribution to the
humanity by saving a life</p>
</body>
</html>